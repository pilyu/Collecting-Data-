{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008d212a",
   "metadata": {},
   "source": [
    "# 100 Historical Spanish newspaper editions\n",
    "\n",
    "**Henning, M. S...; Ianovitskii, M. S....; Raulli, L. S5352029; Uribe Silva, M.P. S...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c762d7",
   "metadata": {},
   "source": [
    "## Introduction and Background \n",
    "\n",
    "Newspapers are the tangible record of the history lived by a society, and their importance lies in the way they observe, describe and record the facts. How do they record reality? How have editorial lines and political trends marked these records? Can we observe these changes of records and trends through a distant observation analysis? Our project wants to provide a dataset of historical Spanish newspaper editions. This dataset should be suitable for tackling our underlying question: **What influences newspaper style?** (the question will be properly adressed in the final Tool and Methods project.)\n",
    "\n",
    "The project focuses on Spain because we wanted to work with a non-English dataset. There is a strong focus within the Digital Humanities on English data - understandably, since it is an international field of study. Still, we wanted to provide a different dataset here. The project also focuses on the years 1890 to 1940. This is a subjective selection, but those years were definitely eventful in Spain: The country moved from a monarchy to a monarchy with a dictatorship to a republic, had a civil war, a revolution and then became a dictatorship. We were interested in seeing whether these tumultous events would be visible in newspaper style.\n",
    "\n",
    "Political change through time is obviously not the only thing that could influence a newspaper edition's style. A language itself changes with **time**, a newspaper's staff as well. All of this would mean that time would be feature we wanted to track for our newspaper editions. Another feature would be their **ideology**: maybe Socialists wrote differently than Conservatives, Anarchists differently from Republicans? Languages are also not monolithic, most have many dialects spoken across a country. Taking this into account, we also wanted to track newspapers' **headquarters** and target **regions**. Another potential feature could be the age of the target **audience**, as there were many papers aimed specifically at young readers. There are also different kinds of newspapers: some are more concerned with breaking the news, others with providing a regular perspective on things. We therefore also track publication **format**. And finally, maybe it was not so much the time that articles were written in, but the time when a newspaper was founded that influenced its style? For this, we also tracked our newspapers' years of **foundation**. \n",
    "\n",
    "The features we wanted to track therefore were:\n",
    "- Ideology\n",
    "- Headquarters\n",
    "- Audience\n",
    "- Region\n",
    "- Format\n",
    "- Foundation\n",
    "\n",
    "We then collected **100 editions** of various newspapers spread out as much as possible along these features. Our data and our metadata come from the Digital Newspaper Library of the National Library of Spain (__[Hemeroteca Digital](https://hemerotecadigital.bne.es/hd/es/advanced)__), where a diversity of newspapers and magazines with different editorial and political lines published from the year 1840 to the year 2022 can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e779d",
   "metadata": {},
   "source": [
    "## Tutorial \n",
    "\n",
    "\n",
    "### Cleaning of the texts - Selection of the corpus\n",
    "\n",
    "After selecting and downloading the raw texts from the Hemeroteca Digital of the National Library of Spain, the next step was to clean the downloaded texts. Text cleaning is a fundamental step for the development of the dataset and subsequent analysis, therefore, we invested a lot of hours to build a consistent and clean dataset as much as we could to answer our research question. We strongly recommend investing time to clean the raw data to be used for any project in a mindful way.\n",
    "\n",
    "In general, the scanned texts were in acceptable reading condition when we collect them. There are two problems though: 1. There were extra spaces in some words, and 2. Certain characters were not recognized by the OCR.\n",
    "\n",
    "Fixing words with extra spaces manually was not an option because of the amount of data, so we decided to create a function that tries to recompose those words, remove the spaces, and put all the tokens in lowercase. To address the OCR issue instead, we decided to work with a Spanish corpus to remove unrecognized text. An important part of the work was therefore to select which corpus we would work with. After evaluating some available corpora in Spanish, we selected two to test in depth:\n",
    "\n",
    "1. **NLTK \"cess_esp\"** corpus was created in 2007 and contains 188,650 words of Spanish which have been syntactically annotated within the framework of the CESS-ECE project (Syntactically & Semantically Annotated Corpora, Spanish, Catalan, Basque). The CESS-ESP was annotated using constituents and functions (with AGTK, University of Pennsylvania). Source: __[Universal Catalogue](http://universal.elra.info/product_info.php?cPath=42_43&products_id=1509)__ <br> <br>\n",
    "2. **SPACY \"es_core_news_md** is a free, open-source library for advanced Natural Language Processing (NLP) in Python, it is designed specifically for production use and it helps you build applications that process large volumes of text. During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation, and so on. This is done by applying rules specific to each language, in our case, we select a Spanish pipeline optimized for CPU. Source: __[Spacy.io](https://spacy.io/models/es)__\n",
    "\n",
    "But how could we quantitatively evaluate which of the corpora best optimizes our future dataset? We decided to select the corpus which would recognize the highest quantity of words in our dataset. For this purpose, we have created a function that, starting from our full dataset, calculates the percentage of tokens recognized by each corpus, and we selected the one with the highest score.\n",
    "\n",
    "The main differences between NLTK and spaCy are well described in __[this article](https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words recognized by each corpus\n",
    "corpora = {}\n",
    "words_corpus = {word.lower() for word in nltk.corpus.cess_esp.words()}\n",
    "corpora[\"cess_esp\"] = {word for word in tokens if word in words_corpus}\n",
    "doc = spacy_es(\" \".join(tokens))\n",
    "corpora[\"es_core_news_md\"] = {token.text for token in doc if token.has_vector}\n",
    "\n",
    "#Calculate the percentage of words recognized \n",
    "for corpus, tokens_recognized in corpora.items():\n",
    "    percent_score = len(tokens_recognized)*100/ len(tokens)\n",
    "    print(f\"{corpus} {percent_score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df523621",
   "metadata": {},
   "source": [
    "This function can be applied to any corpus, which makes it flexible for future use. The advantages of applying this method are mainly that it is simple, relatively fast, and that it allows us to maximize the amount of useful data we can work with, which gives us the possibility to carry out a more extensive analysis of our dataset. Its disadvantage is that we cannot be sure of the quality of the result, since not necessarily a greater amount of data (in this case, words) is synonymous with a higher data quality. Despite the limits, we believe that it is useful for our project to carry out this evaluation and that this method can be improved over time and adjusted to the demands, needs, and objectives of the project that decides to make use of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy nltk\n",
    "!python -m spacy download es_core_news_md\n",
    "import nltk\n",
    "import spacy\n",
    "import os.path\n",
    "from glob import glob\n",
    "spacy_es = spacy.load(\"es_core_news_md\")\n",
    "spacy_es.max_length = 4000000 #Increase the limit of the text that Spacy can work with\n",
    "nltk.download(\"punkt\") \n",
    "\n",
    "#Run this function to read the file\n",
    "def read_file(filename):\n",
    "    with open(filename, encoding='utf8') as infile:\n",
    "        contents = infile.read()\n",
    "    return contents\n",
    "\n",
    "#Run this function to recompose the words, remove the spaces, and put tokens in lower letters\n",
    "def retokenize(tokenized_text):\n",
    "    tokenized = []\n",
    "    is_broken_word = False\n",
    "    temp_word = \"\"\n",
    "    for token in tokenized_text:\n",
    "        token_lower = token.lower()\n",
    "        if len(token) == 1:\n",
    "            if not is_broken_word:\n",
    "                is_broken_word = True\n",
    "                temp_word = token_lower\n",
    "            else:\n",
    "                temp_word += token_lower\n",
    "        else:\n",
    "            if is_broken_word:\n",
    "                tokenized.append(temp_word)\n",
    "                temp_word = \"\"\n",
    "                is_broken_word = False\n",
    "            tokenized.append(token_lower)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed983dc3",
   "metadata": {},
   "source": [
    "Up to this point, all the symbols and unrecognized words that come from the OCR are still included in the dataset, so we are going to remove the ones that are not present in the “es_core_news_md” corpus that we selected earlier. This way, we know that we have only valid words. The trade-off is that we lose some words and all phrase structure, which means that we know that our analysis will be not completely accurate and cannot fully rely on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f81c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function to get clean texts\n",
    "for filename in glob('texts/*.txt'):\n",
    "    basename = os.path.basename(filename)\n",
    "    new_name = f'texts_clean/{basename}'\n",
    "    text = read_file(filename)\n",
    "    tokenized_text = nltk.tokenize.word_tokenize(text)\n",
    "    tokenized_text = retokenize(tokenized_text)\n",
    "    doc = spacy_es(\" \".join(tokenized_text))\n",
    "    clean_texts = [token.text for token in doc if token.has_vector]\n",
    "    with open(new_name, 'w', encoding = 'utf-8') as fp:\n",
    "        for item in clean_texts:\n",
    "            fp.write(f'{item} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4037ea2",
   "metadata": {},
   "source": [
    "### Prepare corpus for stylistic analyis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49871b1",
   "metadata": {},
   "source": [
    "As mentioned in the introduction, this dataset was created to perform some analyses using Stylo and Gephi. In order to make the results easier to read, we needed to rename the files according to the analysis they were intended for. This is the way we did it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a356aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "editions = []\n",
    "for edition in glob('./texts_clean/*.txt'):\n",
    "    editions.append(edition)\n",
    "    \n",
    "def create_subcorpus(feature):\n",
    "    if not os.path.exists(\"./stylo/\"):\n",
    "        os.makedirs(\"./stylo/\")\n",
    "    subcorpus = str(\"./stylo/\" + feature + \"/corpus\")\n",
    "    if not os.path.exists(subcorpus):\n",
    "        os.makedirs(subcorpus)\n",
    "    for edition in editions:\n",
    "        edition_id = int(os.path.splitext(basename(edition))[0])\n",
    "        feature_lookup = str(editions_full.iloc[edition_id-1][feature])\n",
    "        edition_id = str(edition_id)\n",
    "        with open(edition, \"r\", encoding = \"utf8\") as oldfile, open(subcorpus + \"/\" + feature_lookup + \"_\" + edition_id + \".txt\", \"w\", encoding = \"utf8\") as newfile:\n",
    "            for line in oldfile:\n",
    "                newfile.write(line)\n",
    "        \n",
    "create_subcorpus(\"newspaper\")\n",
    "create_subcorpus(\"ideology\")\n",
    "create_subcorpus(\"hq\")\n",
    "create_subcorpus(\"audience\")\n",
    "create_subcorpus(\"region\")\n",
    "create_subcorpus(\"format\")\n",
    "create_subcorpus(\"foundedin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e1a3d",
   "metadata": {},
   "source": [
    "### Explore, analyse, and visualize the data\n",
    "At this point we felt we had all the tools we needed to start with our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546da24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e175bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>ideology</th>\n",
       "      <th>headquarter</th>\n",
       "      <th>audience</th>\n",
       "      <th>region</th>\n",
       "      <th>format</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>link</th>\n",
       "      <th>newspaper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ahora</td>\n",
       "      <td>socialist</td>\n",
       "      <td>madrid</td>\n",
       "      <td>adult</td>\n",
       "      <td>national</td>\n",
       "      <td>daily</td>\n",
       "      <td>1931</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>https://hemerotecadigital.bne.es/hd/es/viewer?...</td>\n",
       "      <td>. autómata que hace un par de años al público ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ahora</td>\n",
       "      <td>socialist</td>\n",
       "      <td>madrid</td>\n",
       "      <td>adult</td>\n",
       "      <td>national</td>\n",
       "      <td>daily</td>\n",
       "      <td>1932</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>https://hemerotecadigital.bne.es/hd/es/viewer?...</td>\n",
       "      <td>nota de gran jornada republicana del ha la rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ahora</td>\n",
       "      <td>socialist</td>\n",
       "      <td>madrid</td>\n",
       "      <td>adult</td>\n",
       "      <td>national</td>\n",
       "      <td>daily</td>\n",
       "      <td>1932</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>https://hemerotecadigital.bne.es/hd/es/viewer?...</td>\n",
       "      <td>del la banda e de la de la al y republicano do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ahora</td>\n",
       "      <td>socialist</td>\n",
       "      <td>madrid</td>\n",
       "      <td>adult</td>\n",
       "      <td>national</td>\n",
       "      <td>daily</td>\n",
       "      <td>1935</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>https://hemerotecadigital.bne.es/hd/es/viewer?...</td>\n",
       "      <td>consecuencia los que últimamente algunas zonas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ahora</td>\n",
       "      <td>socialist</td>\n",
       "      <td>madrid</td>\n",
       "      <td>adult</td>\n",
       "      <td>national</td>\n",
       "      <td>daily</td>\n",
       "      <td>1935</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>https://hemerotecadigital.bne.es/hd/es/viewer?...</td>\n",
       "      <td>concentraciones de tropas en y movimiento de a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id newspaper   ideology headquarter audience    region format  year  month  \\\n",
       "0   1     ahora  socialist      madrid    adult  national  daily  1931      4   \n",
       "1   2     ahora  socialist      madrid    adult  national  daily  1932      4   \n",
       "2   3     ahora  socialist      madrid    adult  national  daily  1932      9   \n",
       "3   4     ahora  socialist      madrid    adult  national  daily  1935      1   \n",
       "4   5     ahora  socialist      madrid    adult  national  daily  1935     11   \n",
       "\n",
       "   day                                               link  \\\n",
       "0    9  https://hemerotecadigital.bne.es/hd/es/viewer?...   \n",
       "1   15  https://hemerotecadigital.bne.es/hd/es/viewer?...   \n",
       "2   16  https://hemerotecadigital.bne.es/hd/es/viewer?...   \n",
       "3    9  https://hemerotecadigital.bne.es/hd/es/viewer?...   \n",
       "4   15  https://hemerotecadigital.bne.es/hd/es/viewer?...   \n",
       "\n",
       "                                      newspaper_text  \n",
       "0  . autómata que hace un par de años al público ...  \n",
       "1  nota de gran jornada republicana del ha la rev...  \n",
       "2  del la banda e de la de la al y republicano do...  \n",
       "3  consecuencia los que últimamente algunas zonas...  \n",
       "4  concentraciones de tropas en y movimiento de a...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editions_with_text = pd.read_csv ('./editions_with_text.csv')\n",
    "editions_with_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6e96e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>ideology</th>\n",
       "      <th>hq</th>\n",
       "      <th>audience</th>\n",
       "      <th>region</th>\n",
       "      <th>format</th>\n",
       "      <th>hq latitude</th>\n",
       "      <th>hq longitude</th>\n",
       "      <th>foundedin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ahora</td>\n",
       "      <td>socialist</td>\n",
       "      <td>madrid</td>\n",
       "      <td>adult</td>\n",
       "      <td>national</td>\n",
       "      <td>daily</td>\n",
       "      <td>40.416.775</td>\n",
       "      <td>-3.703.790</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>andalucia</td>\n",
       "      <td>regionalist</td>\n",
       "      <td>sevilla</td>\n",
       "      <td>adult</td>\n",
       "      <td>andalucia</td>\n",
       "      <td>monthly</td>\n",
       "      <td>37.392.529</td>\n",
       "      <td>-5.994.072</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ceda</td>\n",
       "      <td>carlist</td>\n",
       "      <td>madrid</td>\n",
       "      <td>adult</td>\n",
       "      <td>national</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>40400</td>\n",
       "      <td>-3.703.790</td>\n",
       "      <td>1933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>el amigo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>madrid</td>\n",
       "      <td>youth</td>\n",
       "      <td>national</td>\n",
       "      <td>daily</td>\n",
       "      <td>40.416.775</td>\n",
       "      <td>-3500</td>\n",
       "      <td>1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>el cantábrico</td>\n",
       "      <td>socialist</td>\n",
       "      <td>santander</td>\n",
       "      <td>adult</td>\n",
       "      <td>cantabria</td>\n",
       "      <td>weekly</td>\n",
       "      <td>43.462.774</td>\n",
       "      <td>-3.805.000</td>\n",
       "      <td>1895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      newspaper     ideology         hq audience     region  \\\n",
       "0           0          ahora    socialist     madrid    adult   national   \n",
       "1           1      andalucia  regionalist    sevilla    adult  andalucia   \n",
       "2           2           ceda      carlist     madrid    adult   national   \n",
       "3           3       el amigo          NaN     madrid    youth   national   \n",
       "4           4  el cantábrico    socialist  santander    adult  cantabria   \n",
       "\n",
       "     format hq latitude hq longitude  foundedin  \n",
       "0     daily  40.416.775   -3.703.790       1930  \n",
       "1   monthly  37.392.529   -5.994.072       1916  \n",
       "2  biweekly       40400   -3.703.790       1933  \n",
       "3     daily  40.416.775        -3500       1915  \n",
       "4    weekly  43.462.774   -3.805.000       1895  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspapers = pd.read_csv ('./newspapers.csv')\n",
    "newspapers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd6032",
   "metadata": {},
   "source": [
    "## Active Learning Exercises\n",
    "\n",
    "The dataset containing information about Spanish newspapers from 1890 to 1940 not only deserves a contribution to be expanded and continuously updated but also allows tools such as Stylo and Gephi to be used to draw interesting observations. <br>\n",
    "<br> Above in the Tutorial section you may find what we have created so far, now it is your turn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986c6ed",
   "metadata": {},
   "source": [
    "### Update the DataFrames\n",
    "\n",
    "Given the breadth of resources there is the possibility to update the newspapers data with more newspapers editions, and possibly adding columns with additional information useful to understand Spanish journalistic culture (i.e. name of the journalists). At the moment, there does not seem to be the possibility of downloading the web page via html, but if they were to change the structure, the process of downloading the various editions could be automated. <br>\n",
    "<br> Download from the __[Digital Newspaper Library](https://hemerotecadigital.bne.es/hd/es/advanced)__ of the National Library of Spain other newspaper editions, clean them up using python and add related information to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b360baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a40ac1",
   "metadata": {},
   "source": [
    "### Explore the DataFrames\n",
    "\n",
    "We decided to work with separate datasets because we figured it was tidier this way, but to better visualise all the data you might want to have a unique dataset with all the information. We have created one for you called editions_text_hq, resulted from the merging of the main DataFrame editions_with_text and some columns of the newspapers one that reports the information about the headquarters of the newspapers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a706809",
   "metadata": {},
   "outputs": [],
   "source": [
    "editions_text_hq = editions_with_text.merge(pd.DataFrame(newspapers[['newspaper', 'hq latitude', 'hq longitude',\\\n",
    "                                                                     'foundedin']]), on='newspaper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3604df",
   "metadata": {},
   "source": [
    "Time to create interesting data visualisations! The task is yours, be creative. As an inspiration: how many sample for each ideology? Is there one you should amend by adding samples? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "12449937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8249f",
   "metadata": {},
   "source": [
    "### Come up with interesting research questions\n",
    "\n",
    "Being that this dataset lends itself well to the use of tools such as Stylo and Gephi, one might come in touch with interesting discoveries in the changing of writing styles over time or ideologies (i.e. Which factor influences the style of a newspaper the most? Can you think of other features to add to the dataset?). To do so, we advise you to rename the files each time with the feature you want to investigate, like this: ideology_id, audience_id etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f0b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
